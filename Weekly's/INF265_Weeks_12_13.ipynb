{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "second-garbage",
   "metadata": {},
   "source": [
    "# Weeks 12-13: Object localization\n",
    "\n",
    "## General instructions\n",
    "\n",
    "Every two weeks you will be given an assignment related to the associated module. You have roughly two weeks to complete and submit each of them. There are three weekly group sessions available to help you complete the assignments. Attendance is not mandatory but recommended. However, all assignments are graded and not submitting them or submitting them after the deadline will give you no points.\n",
    "\n",
    "**FORMAT**: Jupyter notebook **(single file, not in a zip please!)**\n",
    "\n",
    "**DEADLINE**: Sunday 11th April, 23:59\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The objective of this assignment is to get a basic understanding of the core concepts in object detection using convolutional neural networks. More specifically, this work focuses on the simpler problem of object localization, where each image contains a single object that should be spatially localized and classified. While object localization is simpler than object detection in the sense that object detection is more general as it allows for multiple objects per scene, object localization remains a rather avanced method, perfectly suited for our current level. This exercise consists of two parts:\n",
    "- To begin with, we will get familiar with the sliding window algorithm, and notice that applying a convolutional neural network sequentially to a specific set of windows within an image is in fact mathematically equivalent to passing the whole image as input for the convolutional neural network. We will in particular try to understand what parameters determine which set of windows will be processed by the convolutional sliding window algorithm and we will run a performance comparison between the sequential and convolutional implementations.\n",
    "- In a second part, we will implement in pytorch our own take on the object localization algorithm. For that purpose, the knowledge acquired in the first part of this assignment will be helpful, as we will try to implement object localization using a convolutional sliding window approach. In order for us to be able to train and test our network even on modest hardware, this exercise will have you work on a custom dataset: you will use the MNIST dataset as a backbone to generate new images with digits randomly placed and transformed, and each image will contain a single digit. While solving object localization on this custom dataset will be much easier than traditional object detection on large datasets such as COCO, it will still require clever vectorized coding if you hope to train your network in a reasonable amount of time !\n",
    "\n",
    "## Andrew's Videos related to this week's assignment\n",
    "\n",
    "- [C4W3L01 Object Localization](https://www.youtube.com/watch?v=GSwYGkTfOKk&list=PL_IHmaMAvkVxdDOBRg2CbcJBq9SY7ZUvs)\n",
    "- [C4W3L03 Object Detection](https://www.youtube.com/watch?v=5e5pjeojznk&list=PL_IHmaMAvkVxdDOBRg2CbcJBq9SY7ZUvs&index=3)\n",
    "- [C4W3L04 Convolutional Implementation Sliding Window](https://www.youtube.com/watch?v=XdsmlBGOK-k&list=PL_IHmaMAvkVxdDOBRg2CbcJBq9SY7ZUvs&index=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-request",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "upper-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-shooting",
   "metadata": {},
   "source": [
    "# Convolutional sliding window\n",
    "\n",
    "## I commented out a lot of printing for task 1 to reduce the size of the notebook, for task to I manage to get the same output dimension, but not the same norm, I think this might be because of weight initialization.\n",
    "\n",
    "The sliding window algorithm refers to any procedure that applies a certain function (\"map\") to sliding windows within an image. In its typical forumaltion, this seems to be an inherently sequential algorithm: process the first window, then the second etc$\\dots$ assuming the function to apply on every window is a convolutional neural network, an interesting fact arises: the sequential implemention of sliding window for this convolutional neural network map becomes exactly equivalent to passing the whole image as input to the convolutional neural network. We will investigate this fact further in this section. Answer the following questions:\n",
    "\n",
    "1) Watch Andrew's video about convolutional sliding window implementation.\n",
    "\n",
    "2) What slight modification is needed on the convolutional neural network such that sequential and convolutional sliding window procedures for this convolutional neural network map become equivalent ?\n",
    "    - the 16 to 400 , with 5x5 filter conv layer needs to work like flatten()\n",
    "3) Reproduce the convolutional neural network from Andrew's video (1:20), and verify that for an input tensor with spatial dimension $16\\times 16$, convolutional sliding window for that convolutional neural network map is the same as a sequential sliding window with stride $2$ along both spatial dimension. You can for instance compute the norm of the difference between the output tensors from convolutional and sequential implementations and check that this norm is indeed $0.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class SeqSliding(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stride = 2\n",
    "        self.layers = []\n",
    "        self.layers.append(nn.Conv2d(3, 16, (5, 5)))\n",
    "        self.layers.append(nn.MaxPool2d((2,2), stride=2))\n",
    "        self.layers.append(nn.Flatten())\n",
    "        self.layers.append(nn.Linear(400, 400))\n",
    "        self.layers.append(nn.Linear(400, 4))\n",
    "        self.layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "    def add_layer(self, layer, idx):\n",
    "        self.layers.insert(idx, layer)\n",
    "\n",
    "    def bad_weight_init(self):\n",
    "        nn.init.constant_(self.layers[0].weight.data, 0.2)\n",
    "        nn.init.constant_(self.layers[3].weight.data, 0.2)\n",
    "        nn.init.constant_(self.layers[4].weight.data, 0.2)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for layer in self.layers:\n",
    "            a = layer(a)\n",
    "        a = torch.reshape(a, (x.shape[0], a.shape[0]//x.shape[0], a.shape[1]))\n",
    "        return a\n",
    "\n",
    "\n",
    "\n",
    "class ConvSliding(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        self.layers.append(nn.Conv2d(3, 16, (5, 5)))\n",
    "        self.layers.append(nn.MaxPool2d((2,2), stride=2))\n",
    "        self.layers.append(nn.Conv2d(16, 400, (5, 5)))\n",
    "        self.layers.append(nn.Conv2d(400, 400, (1, 1)))\n",
    "        self.layers.append(nn.Conv2d(400, 4, (1, 1)))\n",
    "        self.layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "\n",
    "\n",
    "    def bad_weight_init(self):\n",
    "        nn.init.constant_(self.layers[0].weight.data, 0.002)\n",
    "        nn.init.constant_(self.layers[2].weight.data, 0.04)\n",
    "        nn.init.constant_(self.layers[3].weight.data, 0.002)\n",
    "        nn.init.constant_(self.layers[4].weight.data, 0.002)\n",
    "\n",
    "    def add_layer(self, layer, idx):\n",
    "        self.layers.insert(idx, layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for layer in self.layers:\n",
    "            a = layer(a)\n",
    "        return a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def split_to_windows(stride, a, window_size=14):\n",
    "    dims = ((a.shape[2] - window_size)// stride + 1) * ((a.shape[3] - window_size)// stride + 1) * a.shape[0]\n",
    "    new_a = torch.empty((dims, 3, window_size, window_size))\n",
    "    idx = 0\n",
    "    for i in range(a.shape[0]):\n",
    "        for x in range(((a.shape[2] - window_size)// stride + 1)):\n",
    "            for y in range(((a.shape[3] - window_size)// stride + 1)):\n",
    "                new_a[idx] = torch.narrow(torch.narrow(a[i], -2, y*stride, window_size), -1, x*stride, window_size)\n",
    "                idx += 1\n",
    "    return new_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "x1 = torch.rand((1, 3, 14, 14))\n",
    "t1 = torch.rand((1, 3, 16, 16))\n",
    "t_mod = split_to_windows(2, t1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "conv_sliding = ConvSliding()\n",
    "window_sliding = SeqSliding()\n",
    "# conv_sliding.bad_weight_init()\n",
    "# window_sliding.bad_weight_init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 2, 2])\n",
      "tensor([[[[0.2319, 0.2257],\n",
      "          [0.2251, 0.2260]],\n",
      "\n",
      "         [[0.2688, 0.2639],\n",
      "          [0.2656, 0.2654]],\n",
      "\n",
      "         [[0.2648, 0.2729],\n",
      "          [0.2766, 0.2726]],\n",
      "\n",
      "         [[0.2345, 0.2376],\n",
      "          [0.2327, 0.2360]]]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([4, 4, 1, 1])\n",
      "tensor([[[[0.2319]],\n",
      "\n",
      "         [[0.2688]],\n",
      "\n",
      "         [[0.2648]],\n",
      "\n",
      "         [[0.2345]]],\n",
      "\n",
      "\n",
      "        [[[0.2251]],\n",
      "\n",
      "         [[0.2656]],\n",
      "\n",
      "         [[0.2766]],\n",
      "\n",
      "         [[0.2327]]],\n",
      "\n",
      "\n",
      "        [[[0.2257]],\n",
      "\n",
      "         [[0.2639]],\n",
      "\n",
      "         [[0.2729]],\n",
      "\n",
      "         [[0.2376]]],\n",
      "\n",
      "\n",
      "        [[[0.2260]],\n",
      "\n",
      "         [[0.2654]],\n",
      "\n",
      "         [[0.2726]],\n",
      "\n",
      "         [[0.2360]]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[[0.2319, 0.2688],\n",
      "          [0.2648, 0.2345]],\n",
      "\n",
      "         [[0.2251, 0.2656],\n",
      "          [0.2766, 0.2327]],\n",
      "\n",
      "         [[0.2257, 0.2639],\n",
      "          [0.2729, 0.2376]],\n",
      "\n",
      "         [[0.2260, 0.2654],\n",
      "          [0.2726, 0.2360]]]], grad_fn=<ViewBackward>)\n",
      "torch.Size([4, 1, 4])\n",
      "tensor([[[0.2436, 0.1976, 0.2980, 0.2608]],\n",
      "\n",
      "        [[0.2546, 0.2005, 0.2845, 0.2605]],\n",
      "\n",
      "        [[0.2656, 0.2177, 0.2741, 0.2426]],\n",
      "\n",
      "        [[0.2550, 0.2034, 0.2786, 0.2630]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "oc2 = conv_sliding(t1)\n",
    "print(oc2.shape)\n",
    "print(oc2)\n",
    "oc_mod = conv_sliding(t_mod)\n",
    "print(oc_mod.shape)\n",
    "print(oc_mod)\n",
    "print(torch.reshape(oc_mod, (1, 4, 2, 2)))\n",
    "wc_mod = window_sliding(t_mod)\n",
    "print(wc_mod.shape)\n",
    "print(wc_mod)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0030, grad_fn=<CopyBackwards>)\n",
      "tensor(1.0070, grad_fn=<CopyBackwards>)\n",
      "tensor(-0.0040, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n = torch.reshape(oc_mod, (4, 4))\n",
    "q = torch.reshape(wc_mod, (4, 4))\n",
    "print(torch.linalg.norm(n))\n",
    "print(torch.linalg.norm(q))\n",
    "print(torch.linalg.norm(n) - torch.linalg.norm(q))\n",
    "# print(torch.linalg.norm(n-q))\n",
    "# print(n)\n",
    "# print(n - q)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I can see that the math corresponds, but I don't get the answers to match completely because the indexing is different\n",
    "between my splitting and the convolutional splitting, where window sequential[0][0] = (convolutional[0][0][0][0], convolutional[0][1][0][0], convolutional[0][2][0][0], convolutional[0][3][0][0])\n",
    "Tested with some weight initialisation and got the same answers on sequential and convolutional."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4) What happens when the input tensor now has spatial dimensions $28\\times 28$? Try a few other spatial dimensions. Can you find some such that convolutional and sequential procedures yield different outputs ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8, 8])\n",
      "torch.Size([64, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand((1, 3, 28, 28))\n",
    "t_mod = split_to_windows(2, t1)\n",
    "oc2 = conv_sliding(t1)\n",
    "print(oc2.shape)\n",
    "# print(oc2)\n",
    "wc_mod = window_sliding(t_mod)\n",
    "print(wc_mod.shape)\n",
    "# print(wc_mod)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5) Do a time comparison for various input spatial dimensions between convolutional and sequential sliding window procedures for Andrew's convolutional network map. Which is the most efficient ? Can you explain why ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import  time\n",
    "\n",
    "def time_model():\n",
    "    resolutions = [16, 28, 720, 1080]\n",
    "    for res in resolutions:\n",
    "        data = torch.rand((5, 3, res, res))\n",
    "        data_mod = split_to_windows(2, data)\n",
    "        start = time.time()\n",
    "        test_seq = window_sliding(data_mod)\n",
    "        stop = time.time()\n",
    "        print(\"Sequential time: \", stop-start, \" input dims: \", res, \" output dims: \" ,test_seq.shape)\n",
    "        start = time.time()\n",
    "        test_seq = conv_sliding(data)\n",
    "        stop = time.time()\n",
    "        print(\"Conv time: \", stop-start, \" input dims: \", res, \" output dims: \" ,test_seq.shape)\n",
    "\n",
    "# time_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6) Modify slightly Andrew's convolutional neural network such that there is now an additional convolutional layer. Do convolutional and sequential sliding window with stride $2$ procedures still coincide ? If not, what should be the stride for them to coincide again ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "added_conv_conv_sliding = ConvSliding()\n",
    "added_conv_seq_sliding = SeqSliding()\n",
    "added_conv_conv_sliding.add_layer(nn.Conv2d(16, 16, (5, 5)), 2)\n",
    "added_conv_seq_sliding.add_layer(nn.Conv2d(16, 16, (5, 5)), 2)\n",
    "\n",
    "acx1 = torch.rand((1, 3, 22, 22))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 1, 1])\n",
      "torch.Size([1, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "out = added_conv_conv_sliding(acx1)\n",
    "print(out.shape)\n",
    "out = added_conv_seq_sliding(split_to_windows(2, acx1, window_size=22))\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding Conv layers changes the size of the window. I'm guessing depending on filter size and channel out you can customize\n",
    "that to what you want. I added another 5x5 filter with out 16 channels and it increased the window frow 14 to 22"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "7) Same question but now add an additional max pooling layer (with kernel size $2$) instead."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "added_conv_conv_sliding = ConvSliding()\n",
    "added_conv_seq_sliding = SeqSliding()\n",
    "added_conv_conv_sliding.add_layer(nn.MaxPool2d((2,2), stride=2), 2)\n",
    "added_conv_seq_sliding.add_layer(nn.MaxPool2d((2,2), stride=2), 2)\n",
    "\n",
    "acx1 = torch.rand((1, 3, 28, 28))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 2, 2])\n",
      "torch.Size([4, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "out = added_conv_conv_sliding(acx1)\n",
    "print(out.shape)\n",
    "out = added_conv_seq_sliding(split_to_windows(4, acx1, window_size=24))\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It increases the stride by 2."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "8) It turns out that the architecture of the convolutional neural network map itself will determine what is the set of windows for which convolutional and sequential sliding window procedures coincide for that convolutional neural network map. In the following, let us assume that all convolutional layers have padding $0$ and all max pooling layers have kernel size $2$. Can you guess the formulas that give the number of windows and their indices ? $\\textbf{Hint}$: Remember your implementation of the Conv2d layer. $\\textbf{If you can't find this, CONTACT US!}$ It will be important in the second section.\n",
    "\n",
    "window_reduction_per_conv_layer = (- kernel size + 1) * (number of maxpooling layers before this point + 1)\n",
    "\n",
    "window_reduction_per_pooling_layer = in/2\n",
    "\n",
    "calculate windows for task 6:\n",
    "\n",
    "architecture:\n",
    "\n",
    "- conv(5,5) + 4\n",
    "- max() *2\n",
    "- conv(5,5) + 4\n",
    "- conv(last 5,5)+4\n",
    "- conv(1,1) 1\n",
    "\n",
    "go bottom up:\n",
    "\n",
    "(1 +4 + 4 )* 2 + 4 = 9*2+4=22 Correct :)\n",
    "\n",
    "test with task 1 and 7\n",
    "\n",
    "* 1. (1 + 4) * 2 + 4 = 14\n",
    "\n",
    "* 7. (1+4) * 2 * 2 + 4 = 24\n",
    "\n",
    "* 6. Alternative implementation extra conv layer at the start: (1 + 4) * 2 + 4 + 4 = 18\n",
    "\n",
    "Indecies:\n",
    "First image, first window = [0][0..len(classes)][0][0]\n",
    "Second image, second window = [1][0..len(classes)][1][0]\n",
    "Fourth image, fourth window = [3][0..len(classes)][1][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "eight-springer",
   "metadata": {},
   "source": [
    "# MNIST localization in Pytorch\n",
    "\n",
    "## 1 Data generation\n",
    "\n",
    "In this second section we aim to implement from scratch our custom digit localization algorithm. We will use the MNIST dataset as a backbone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "automatic-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.MNIST.resources = [\n",
    "            ('https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz', 'f68b3c2dcbeaaa9fbdd348bbdeb94873'),\n",
    "            ('https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz', 'd53e105ee54ea40749a09fcbcd1e9432'),\n",
    "            ('https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz', '9fb629c4189551a2d022fa330f9573f3'),\n",
    "            ('https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz', 'ec29112dd5afa0611ce80d1b7f02629c')\n",
    "        ]\n",
    "def load_MNIST(MNIST_path, preprocessor, num_workers):\n",
    "    train_dataset = datasets.MNIST(MNIST_path, train=True, download=True, transform=preprocessor)\n",
    "    val_test_dataset = datasets.MNIST(MNIST_path, train=False, download=True, transform=preprocessor)\n",
    "    n_val_test = len(val_test_dataset.targets)\n",
    "    # The test set will only contain 30 images, perfect for visualization purpose:\n",
    "    val_dataset, test_dataset = torch.utils.data.random_split(val_test_dataset, [n_val_test-30, 30])\n",
    "    MNIST_datasets = {\"train\": train_dataset, \n",
    "                      \"val\": val_dataset,\n",
    "                      \"test\": test_dataset}\n",
    "    MNIST_generators = {\"train\": torch.utils.data.DataLoader(MNIST_datasets[\"train\"], \n",
    "                                                             batch_size=4, \n",
    "                                                             shuffle=True, \n",
    "                                                             num_workers=num_workers),\n",
    "                        \"val\": torch.utils.data.DataLoader(MNIST_datasets[\"val\"], \n",
    "                                                           batch_size=256, \n",
    "                                                           shuffle=False, \n",
    "                                                           num_workers=num_workers),\n",
    "                        \"test\": torch.utils.data.DataLoader(MNIST_datasets[\"test\"], \n",
    "                                                            batch_size=30, \n",
    "                                                            shuffle=False, \n",
    "                                                            num_workers=num_workers)}\n",
    "    return MNIST_datasets, MNIST_generators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-garage",
   "metadata": {},
   "source": [
    "We now want to have a digit generator in pytorch that would produce samples \"on the fly\" for the digit localization task. The problem we have is that all MNIST digits are centered, have the same orientation and roughly the same size which makes the localization task pretty dull. \n",
    "\n",
    "**1) Write a data preprocessor that would do the following:**\n",
    "\n",
    "**a) Randomly rotates digits.**\n",
    "\n",
    "**b) Randomly rescales digits with a scale factor randomly sampled in the range [0.75,1.25].**\n",
    "\n",
    "**c) Randomly places digits within a zeros tensor having spatial dimensions $64\\times 64$.**\n",
    "\n",
    "**You can make sure your data generator works as intended by visualizing the test set generated by MNIST_generators[\"test\"]. Each image generated this way should contain one and exactly one digit!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "intensive-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(12)\n",
    "\n",
    "def padRandom(image):\n",
    "    x_rand_pad = random.randrange(64 - image.size[0])\n",
    "    y_rand_pad = random.randrange(64 - image.size[1])\n",
    "    image = transforms.functional.pad(image, [x_rand_pad, y_rand_pad, 64-image.size[0]-x_rand_pad, 64-image.size[1]-y_rand_pad])\n",
    "    return image\n",
    "\n",
    "MNIST_path = '../Data/'\n",
    "preprocessor = transforms.Compose(\n",
    "                [transforms.Lambda(padRandom),\n",
    "                 transforms.RandomRotation(90),\n",
    "                 transforms.ToTensor()]\n",
    "                )\n",
    "num_workers = 0\n",
    "MNIST_datasets, MNIST_generators = load_MNIST(MNIST_path, preprocessor, num_workers)\n",
    "elem = next(iter(MNIST_generators[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x18ad106e730>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(elem[0][4].permute(1, 2, 0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "afraid-killing",
   "metadata": {},
   "source": [
    "## 2 Convolutional sliding window\n",
    "\n",
    "We then need to implement the convolutional neural network that will do the heavy work. In a nutshell, we want to have a CNN that is able to locate and classify digits within input images with base dimensions $(h\\_in, w\\_in)$. Then, we use this CNN as the backbone mapping in a sliding window procedure, in order to be able to locate and classify digits within larger images with dimensions $(H\\_in, W\\_in)$ such that $H\\_in > h\\_in$ and $W\\_in > w\\_in$. In the first section, you learned how to perform this efficiently by converting the sequential sliding window procedure into a convolutional one. More specifically, you learned that passing directly the whole large image to the CNN mapping gives an output tensor whose spatial dimensions $(H\\_out, W\\_out)$ correspond to the number of sliding windows in the sequential procedure such that each cell is exactly the output of the CNN applied to the corresponding window. Moreover, you also are able to find the indices of these windows. \n",
    "\n",
    "**2) Use this knowledge in order to implement the CNN you'll be using for the digit localization task. Remember that the window size $(h\\_in, w\\_in)$ should be large enough to contain digits entirely and the input size $(H\\_in, W\\_in)$ should match the dimensions of the tensors produced by you generator, namely $64\\times 64$.**\n",
    "\n",
    "Two proposed architecture:\n",
    "\n",
    "Stride=2\n",
    "1. conv(5,5)\n",
    "1. conv(3,3)\n",
    "1. maxpool(stride=2)\n",
    "1. conv(3,3)\n",
    "1. conv(5,5)\n",
    "1. conv(5,5)\n",
    "1. conv(1,1)\n",
    "1. conv(1,1)\n",
    "\n",
    "\n",
    "Stride=4\n",
    "1. conv(5,5)\n",
    "1. maxpool(stride=2)\n",
    "1. conv(3,3)\n",
    "1. maxpool(stride=2)\n",
    "1. conv(5,5)\n",
    "1. conv(1,1)\n",
    "1. conv(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "\n",
    "class ConvMINSTStride2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        self.layers.append(nn.Conv2d(1, 16, (5, 5)))\n",
    "        self.layers.append(nn.Conv2d(16, 32, (3, 3)))\n",
    "        self.layers.append(nn.MaxPool2d((2,2), stride=2))\n",
    "        self.layers.append(nn.Conv2d(32, 16, (3, 3)))\n",
    "        self.layers.append(nn.Conv2d(16, 400, (5, 5)))\n",
    "        self.layers.append(nn.Conv2d(16, 400, (5, 5)))\n",
    "        self.layers.append(nn.Conv2d(400, 400, (1, 1)))\n",
    "        self.layers.append(nn.Conv2d(400, 4, (1, 1)))\n",
    "        self.layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for layer in self.layers:\n",
    "            a = layer(a)\n",
    "        return a\n",
    "\n",
    "\n",
    "class ConvMINSTStride4(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        self.layers.append(nn.Conv2d(1, 16, (5, 5)))\n",
    "        self.layers.append(nn.MaxPool2d((2,2), stride=2))\n",
    "        self.layers.append(nn.Conv2d(16, 16, (3, 3)))\n",
    "        self.layers.append(nn.MaxPool2d((2,2), stride=2))\n",
    "        self.layers.append(nn.Conv2d(16, 400, (5, 5)))\n",
    "        self.layers.append(nn.Conv2d(400, 400, (1, 1)))\n",
    "        self.layers.append(nn.Conv2d(400, 4, (1, 1)))\n",
    "        self.layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for layer in self.layers:\n",
    "            a = layer(a)\n",
    "        return a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Bounding box extraction, encoding\n",
    "\n",
    "At the moment, we only have access to the digit labels. This is not sufficient for the task of digit localization: we need those bounding boxes, so we will produce them ourselves ! \n",
    "\n",
    "**3-1) Implement a code that will produce sharp bounding boxes around a digit. You can for instance use torch.where(... > 0.0) to find the limits of the digit in an image.**\n",
    "\n",
    "We will encode the digit localization problem using a target vector of size $6$: $y = (y_0, y_1, y_2, y_3, y_4, y_5)$:\n",
    "- $y_0$ is binary and encodes whether or not a digit was detected.\n",
    "- $y_1$ is categorical and encodes the label of the digit (if any).\n",
    "- $y_2, y_3, y_4, y_5$ are continuous and encode the bounding box of the digit (if any).\n",
    "\n",
    "We want every window to carry information. As such, we want to produce one $y$ for every window, such that the groudtruth tensor $Y\\_true$ should have dimensions $(N,6,H\\_out,W\\_out)$. In order to do this, for each window we can check if the digit is visible in this window:\n",
    "- If that's the case, $y_0$ is $1$, $y_1$ is the label of the digit and $y_2, y_3, y_4, y_5$ are the bounding box values **converted in the referential of the window**: the top of the window $h\\_start$ is treated as $0.0$ wheras the bottom of the window $h\\_end$ is treated as $1.0$ (and similarly for the left and right of the window). \n",
    "- Otherwise, the $y$ for that window is simply a zeros vector (if that's the case, we only really care about $y_0=0$).\n",
    "\n",
    "**3-2) Implement the encoding procedure. The code may have a similar structure as in the cell below:**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "impossible-farmer",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TODO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-18-377cf762ce7d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mH_out\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mTODO\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mW_out\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mTODO\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mbounding_boxes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mN\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m4\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mY_true\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mN\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m6\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mH_out\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mW_out\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mn\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mN\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'TODO' is not defined"
     ]
    }
   ],
   "source": [
    "H_out = [TODO]\n",
    "W_out = [TODO]\n",
    "bounding_boxes = torch.zeros(size=(N, 4))\n",
    "Y_true = torch.zeros(size=(N, 6, H_out, W_out))\n",
    "for n in range(N):\n",
    "    # Bounding box extraction:\n",
    "    bounding_boxes[n,:] = [TODO]\n",
    "    # Bounding box coordinates:\n",
    "    top = bounding_boxes[n,0]\n",
    "    bottom = bounding_boxes[n,1]\n",
    "    left = bounding_boxes[n,2]\n",
    "    right = bounding_boxes[n,3]\n",
    "    for h in range(H_out):\n",
    "        for w in range(W_out):\n",
    "            # Coordinates of the window (h,w):\n",
    "            h_start = [TODO]\n",
    "            w_start = [TODO]\n",
    "            h_end = [TODO]\n",
    "            w_end = [TODO]\n",
    "            # If a digit is visible in the window (h,w) <==>\n",
    "            # rectangles (top, bottom, left, right) and (h_start, h_end, w_start, w_end) intersect:\n",
    "            if [TODO]:              \n",
    "                Y_true[n,:,h,w] = [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-china",
   "metadata": {},
   "source": [
    "## 4 Training loop\n",
    "\n",
    "We are now ready to train our model. The only remaining question is the loss. We want a loss of the form $\\sum_{n < N} \\sum_{h < H\\_out} \\sum_{w < W\\_out} L(Y\\_true[n,:,h,w], Y\\_out[n,:,h,w])$,\n",
    "where \n",
    "\n",
    "$L(Y\\_true[n,:,h,w], Y\\_out[n,:,h,w]) = L\\_detection + L\\_classification + L\\_regression$ such that\n",
    "\n",
    "- $L\\_detection = binary\\_cross\\_entropy(sigmoid(Y\\_out[n,0,h,w]), Y\\_true[n,0,h,w])$\n",
    "\n",
    "If $sigmoid(Y\\_out[n,0,h,w]) > 0.5$ and $Y\\_true[n,0,h,w] == 1$ (\"true positive detection of digit in window $(h,w)$\"):\n",
    "- $L\\_classification = negative\\_log\\_likelihood\\_loss(log\\_softmax(Y\\_out[n,1\\!:\\!11,h,w]), Y\\_true[n,1,h,w])$ \n",
    "- $L\\_regression = mse(Y\\_out[n,-4\\!:,h,w], Y\\_true[n,-4\\!:,h,w])$\n",
    "\n",
    "Else:\n",
    "- $L\\_classification=0.0$\n",
    "- $L\\_regression=0.0$\n",
    "\n",
    "Essentially, this loss indicates that we break down the whole task into three subproblems:\n",
    "\n",
    "a) Is there a digit in the current window ?\n",
    "\n",
    "If so,\n",
    "\n",
    "b) What is the digit in the window ?\n",
    "\n",
    "c) Where is the digit in the window ?\n",
    "\n",
    "The two last questions only make sense assuming the first question was positively answered, hence the use of a conditional loss. Notice also that we sum the loss not only over samples in a batch, but also over windows. Essentially, the set of windows forming the sliding window procedure can be seen itself as a minibatch of samples. \n",
    "\n",
    "**4) Implement the training loop, then train your model for a few epochs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-mention",
   "metadata": {},
   "source": [
    "## 5 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-jonathan",
   "metadata": {},
   "source": [
    "Your model should now be trained, great ! But now you may wonder: \"How do I predict ?\". And indeed it is not so obvious. In our implementation, our model should output a tensor $Y\\_out$ with dimensions $(N,15,H\\_out,W\\_out)$, that is we have one prediction vector per window in the sliding window procedure. We propose the following strategy: among all the predicted vectors, keep the one associated to the window with the highest label classification score. In other word, $\\forall n<N$:\n",
    "\n",
    "a) Compute the max classification scores: $max\\_classification\\_scores = max_{i=1:11} \\{log\\_softmax(Y\\_out[n,i,:,:])\\}$.\n",
    "\n",
    "b) Extract the indices of the best window: $h\\_best,w\\_best = argmax_{h<H\\_out,w<W\\_out} \\{max\\_classification\\_scores[h,w]\\}$.\n",
    "\n",
    "c) Save the prediction associated to the best window. **You will have to convert the predicted bounding box back to the referential of the input image.** \n",
    "\n",
    "**5) Implement the predict function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-support",
   "metadata": {},
   "source": [
    "## 6 Visualization\n",
    "\n",
    "Time to test your hard work by visualizing your predictions ! \n",
    "\n",
    "**6) Implement a function that will display the 30 images contained in the test set, with the true bounding boxes in green and the predicted bounding boxes in red overlayed over images. Each image will be titled with its groundtruth and predicted labels. You can use fig, axes = plt.subplots(6, 5, figsize=(20, 16)) in order to place the images conveniently.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}