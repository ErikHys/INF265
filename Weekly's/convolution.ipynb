{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Implementing a convolutional layer\n",
    "\n",
    "#### General instructions\n",
    "\n",
    "Each week you will be given an assignment related to the associated module. You have roughly one week to complete and submit each of them. There are 3 weekly group sessions available to help you complete the assignments. Attendance is not mandatory but recommended. However, assignments are graded each week and not submitting them or submitting them after the deadline will give you no points\n",
    "\n",
    "**FORMAT**: Jupyter notebook  \n",
    "**DEADLINE**: Sunday 28th February, 23:59\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The objective of this assignment is get an in-depth understanding of the convolutional layer as it is a crucial layer in Deep Neural Networks, especially when applied to image dataset. In order to do so, we will simply implement our own custom convolutional layer ``MyConv2d`` and make sure that we get the same results than if we used [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d).\n",
    "\n",
    "\n",
    "## Contents:\n",
    "\n",
    "1. Utils\n",
    "2. Implement a custom layer in Pytorch: ``MyConv2d``\n",
    "3. Use ``MyConv2d`` inside a neural network model\n",
    "4. Test ``MyConv2d`` by comparing it to [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d)\n",
    "\n",
    "## Andrew's Videos related to today's assignment\n",
    "\n",
    "- [C4W1L02 Edge Detection Examples](https://www.youtube.com/watch?v=XuD4C8vJzEQ&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=2)\n",
    "- [C4W1L04 Padding](https://www.youtube.com/watch?v=smHa2442Ah4&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=4)\n",
    "- [C4W1L05 Strided Convolutions](https://www.youtube.com/watch?v=tQYZaDn_kSg&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=5)\n",
    "- [C4W1L06 Convolutions Over Volumes](https://www.youtube.com/watch?v=KTB_OFoAQcc&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=6)\n",
    "- [C4W1L07 One Layer of a Convolutional Net](https://www.youtube.com/watch?v=jPOAS7uCODQ&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=7)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import datetime\n",
    "import numpy as np\n",
    "import collections\n",
    "from typing import Sequence\n"
   ]
  },
  {
   "source": [
    "## 1. Utils\n",
    "\n",
    "Nothing to see in the cell below, just the definition of 3 functions we'll need later, you don't even need to read them, just know that there is:\n",
    "\n",
    "- ``load_MNIST``:\n",
    "- ``training_loop``\n",
    "- ``int_to_pair`` : Return `(n, n)` if `n` is a int or `n` if `n` is already a tuple of length 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu.\n",
      "Files already downloaded and verified\n",
      "Size of the training dataset:  60000\n",
      "Size of the validation dataset:  10000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "print(f\"Device {device}.\")\n",
    "\n",
    "\n",
    "\n",
    "def load_MNIST(data_path='../data/'):\n",
    "    \"\"\"\n",
    "    Return MNIST train and val dataset\n",
    "    \"\"\"\n",
    "    MNIST_train = datasets.MNIST(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,   \n",
    "        transform=transforms.Compose([\n",
    "            transforms.CenterCrop(20),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "        ]))\n",
    "\n",
    "    MNIST_val = datasets.CIFAR10(\n",
    "        data_path, \n",
    "        train=False,      \n",
    "        download=True,   \n",
    "        transform=transforms.Compose([\n",
    "            transforms.CenterCrop(20),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            \n",
    "        ]))\n",
    "\n",
    "    print('Size of the training dataset: ', len(MNIST_train))\n",
    "    print('Size of the validation dataset: ', len(MNIST_val))\n",
    "\n",
    "    return MNIST_train, MNIST_val\n",
    "\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    \"\"\"\n",
    "    Train our model and save weight values\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device) \n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Here we store weight values at each step of the training process (see also last cell and MyNet definition)\n",
    "            model.conv1_weight_values.append(model.conv1.weight.data.clone().detach())\n",
    "            if model.conv1.bias is not None:\n",
    "                model.conv1_bias_values.append(model.conv1.bias.data.clone().detach())\n",
    "            \n",
    "            optimizer.zero_grad()    \n",
    "            \n",
    "            loss_train += loss.item()\n",
    "\n",
    "        print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "            datetime.datetime.now(), epoch,\n",
    "            loss_train / len(train_loader)))\n",
    "\n",
    "def int_to_pair(n):\n",
    "    \"\"\"\n",
    "    Return `(n, n)` if `n` is a int or `n` if `n` is already a tuple of length 2\n",
    "    \"\"\"\n",
    "    # If n is a float or integer\n",
    "    if not isinstance(n, Sequence):\n",
    "        return (int(n), int(n))\n",
    "    elif len(n) == 1:\n",
    "        return (int(n[0]), int(n[0]))\n",
    "    elif len(n) == 2:\n",
    "        return ( int(n[0]), int(n[1]) )\n",
    "    else:\n",
    "        raise ValueError(\"Please give an int or a pair of int\")\n",
    "\n",
    "MNIST_train, MNIST_val = load_MNIST()\n"
   ]
  },
  {
   "source": [
    "## 2. Implement a custom layer in Pytorch: MyConv2d \n",
    "\n",
    "In the cell below, there is a template of a ``MyConv2d`` class that would re-create a [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d) layer. By solving the 4 problems below you will complete this class step by step.\n",
    "\n",
    "First of all, defining a custom layer in PyTorch is very similar to defining a custom neural network or a custom block of layers as they all require to subclass the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=nn%20module#torch.nn.Module) class. (See the 3rd tutorial ``03 - Define a custom deep Neural Network in Pytorch`` for more details about defining your custom neural network). So as usual, we have to create a class that subclasses nn.Module and that implements a [forward](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward) method defining what happens to a given input.\n",
    "\n",
    "\n",
    "## TODO:\n",
    "\n",
    "### 1. Compute the output shape of a convolutional layer\n",
    "\n",
    "In pytorch's documentation, you will often encounter these notations:\n",
    "\n",
    "- ``N``: batch size,         (how many inputs do you feed at the same time)\n",
    "- ``C``: number of channels, (refers to the colors RGB=3, RGBA=4, etc if we are talking about the input or refers to the number of filter if we are talking about the output of a convolutional layers)\n",
    "- ``H``: height of the image\n",
    "- ``W``: width of the image\n",
    "\n",
    "\n",
    "Take a look at the [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d) documentation and scroll down to examine the output shape formula. \n",
    "\n",
    "You can also watch Andrew's video  [C4W1L07 One Layer of a Convolutional Net](https://www.youtube.com/watch?v=jPOAS7uCODQ&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=7) to get an illustration of this formula \n",
    "\n",
    "Write a method ``__get_output_size(self, x)`` (so inside the ``MyConv2d``  class and with ``x`` being an input batch of dimension ``(N, C_in, H_in, W_in)``) that returns the output shape ``(N, C_out, H_out, W_out)`` (following Pytorch's notations) of a convolutional layer whose structure would be defined according to:\n",
    "\n",
    "- ``N = x.shape[0]``\n",
    "- ``C_in = self.in_channels``\n",
    "- ``H_in = x.shape[-2]``\n",
    "- ``W_in = x.shape[-1]``\n",
    "- ``kernel_size = self.kernel_size`` (Note: it's a tuple (kernel_size_height, kernel_size_width))\n",
    "- ``padding = self.padding`` (Note: it's a tuple (padding_height, padding_width))\n",
    "- ``stride = self.stride`` (Note: it's a tuple (stride_height, stride_width))\n",
    "\n",
    "### 2. Apply padding to an image\n",
    "\n",
    "In this video [C4W1L04 Padding](https://www.youtube.com/watch?v=smHa2442Ah4&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=4) you saw how to apply padding to an image\n",
    "\n",
    "Write a method ``__apply_padding(self, x)`` (so inside the ``MyConv2d``  class and with ``x`` being an input batch of dimension ``(N, C_in, H_in, W_in)``) that applies padding to ``x``, i.e. it returns a tensor ``x_pad`` whose center values are the same as ``x`` values but with extra zeros on the border (the numbers of zeros to add are defined by ``self.padding``) \n",
    "\n",
    "**Note** the output value is then of dimension ``(N, C_in, H_in + 2*self.padding[0], W_in + 2*self.padding[1])``\n",
    "\n",
    "### 3. Apply Convolution to an image\n",
    "\n",
    "You saw how to apply convolution to an image, in the following video:\n",
    "\n",
    "- [C4W1L02 Edge Detection Examples](https://www.youtube.com/watch?v=XuD4C8vJzEQ&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=2)\n",
    "- [C4W1L05 Strided Convolutions](https://www.youtube.com/watch?v=tQYZaDn_kSg&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=5)\n",
    "- [C4W1L06 Convolutions Over Volumes](https://www.youtube.com/watch?v=KTB_OFoAQcc&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=6)\n",
    "- [C4W1L07 One Layer of a Convolutional Net](https://www.youtube.com/watch?v=jPOAS7uCODQ&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=7)\n",
    "\n",
    "Write a method ``__apply_conv(self, x_pad, i, j)`` (so inside the ``MyConv2d``  class and with ``x_pad`` of dimension ``(N, C_in, H_in + 2*self.padding[0], W_in + 2*self.padding[1])``) that computes computes the ``[i,j]`` output of the convolutional operation applied to ``x_pad``. Note that since we are to follow Pytorch's implementation of convolution, we can choose to have a bias or not. So in your ``__apply_conv`` there must be somewhere a condition ``if self.bias is None: ...... else: ....... ``\n",
    "\n",
    "**Note**: A few word about vectorization: In python you should always favour vectorized computations because it is much faster. BUT here the most important thing is that you get the right result. So in your  ``__apply_conv`` method you can choose between\n",
    "\n",
    "- Vectorize computations with respect to the number of filter (``C_out`` dimension) and with respect to the batch size (``N`` dimension).\n",
    "- Don't vectorize and have some for loops either inside your ``__apply_conv`` method or your ``forward`` method (next question)\n",
    "Do as you wish.\n",
    "\n",
    "### 4. Implementing a Convolutional layer (forward method)\n",
    "\n",
    "In the [forward](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward) method, combine your previously defined methods to:\n",
    "\n",
    "1. Figure out the output shape expected by calling ``self.__get_output_size``\n",
    "2. Apply padding to ``x`` by calling ``self.__apply_padding`` and store the result in ``x_pad``\n",
    "3. Instantiate an ``out`` tensor with the right shape\n",
    "4. Compute and store in ``out`` the result of the convolution operation by calling ``self.__apply_conv`` (depending on point 3. you might have to have some for loops)\n",
    "5. Return ``out``"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom convolutional 2d layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels:int,\n",
    "        out_channels:int,\n",
    "        kernel_size, \n",
    "        stride = 1, \n",
    "        padding = 0, \n",
    "        bias:bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        in_channels: Number of input channels \n",
    "        out_channels: Number of input channels (number of filters)\n",
    "        kernel_size: Size of your filter (either a int or a tuple of int)\n",
    "        stride: Length of the kernel's jumps  (either a int or a tuple of int)\n",
    "        padding: How many pixels do you add around the border of your image (either a int or a tuple of int)\n",
    "        bias: Should we add a bias parameter to each filter or not?\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        # Will NOT be automatically added to the list\n",
    "        # of trainable parameter (see doc of nn.Parameter)\n",
    "        self.in_channels = int(in_channels)\n",
    "        self.out_channels = int(out_channels)\n",
    "        self.padding = int_to_pair(padding)\n",
    "        self.stride = int_to_pair(stride)\n",
    "        self.kernel_size = int_to_pair(kernel_size)\n",
    "\n",
    "        # Will be automatically added to the list of \n",
    "        # model's trainable parameters (see doc quoted above)\n",
    "        # Dim = (C_out, C_in, kernel_height, kernel_width)\n",
    "        self.weight = nn.Parameter(torch.Tensor(1, self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1]))\n",
    "        # Initialize weight (bad initialization here but we'll re-initialize them later anyway)\n",
    "        self.weight.data = torch.zeros((self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1]))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(self.out_channels))\n",
    "            # Initialize bias (bad initialization here but we'll re-initialize them later anyway)\n",
    "            self.bias.data = torch.zeros((self.out_channels))\n",
    "        else:\n",
    "            self.bias = None\n",
    "            #print(self.bias) # Uncomment this to make sure that the printed output starts with  \"Parameter containing: ...\"\n",
    "        #print(self.weight) # Uncomment this to make sure that the printed output starts with  \"Parameter containing: ...\"\n",
    "\n",
    "\n",
    "    def __get_output_size(self, x):\n",
    "        # shape of x: (N, C_in, H_in, W_in)\n",
    "        # Note: batch_size is the only dimension that is not influenced\n",
    "        # by the convolution operation\n",
    "        return x.shape[0], self.out_channels, int(np.floor(((x.shape[-2] + 2*self.padding[0] - self.kernel_size[0])/self.stride[0]) + 1)), int(np.floor(((x.shape[-1] + 2*self.padding[1] - self.kernel_size[1])/self.stride[1]) + 1))\n",
    "\n",
    "    def __apply_padding(self, x):\n",
    "        return torch.tensor([[[[0 if w-self.padding[1] <= 0 or h-self.padding[0] <= 0 or w >= (x.shape[-1]+2*self.padding[1]-self.padding[1]) or h >= x.shape[-2]+2*self.padding[0]-self.padding[0] else x[n, c, h-self.padding[0], w-self.padding[1]] for w in range(x.shape[-1]+2*self.padding[1])] for h in range(x.shape[-2]+2*self.padding[0])] for c in range(self.in_channels)]for n in range(x.shape[0])])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __apply_conv(self, x_pad, i, j):\n",
    "        t = torch.multiply(torch.narrow(torch.narrow(x_pad , -2, j, self.kernel_size[0]), -1, i, self.kernel_size[1]), self.weight)\n",
    "        if self.bias is None:\n",
    "            return torch.tensor([torch.sum(t[i]) for i in range(self.out_channels)])\n",
    "        return torch.tensor([torch.sum(t[i]) + self.bias[i] for i in range(self.out_channels)])\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Required method for any nn.Module class\n",
    "        \"\"\"\n",
    "        # shape of x: (N, C_in, H_in, W_in)\n",
    "        self.output_size = self.__get_output_size(x)\n",
    "        x_pad = self.__apply_padding(x)\n",
    "        out = torch.zeros(self.output_size)\n",
    "        for n in range(0, self.output_size[0]):\n",
    "            for h in range(0, self.output_size[2], self.stride[0]):\n",
    "                for w in range(0, self.output_size[3], self.stride[1]):\n",
    "                    convolution = self.__apply_conv(x_pad[n], w, h)\n",
    "                    for c in range(self.out_channels):\n",
    "                        out[n, c, h, w] = convolution[c]\n",
    "        return out\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Standard python method to implement if you want to custom your ``print(MyConv2d(...))``\n",
    "\n",
    "        This method is not mandotory, I just wrote it so that you can print your convolutional layer\n",
    "        the same way it is printed when you use a Conv2d layer\n",
    "        \"\"\"\n",
    "        string = (\n",
    "            \"MyConv2d(\" + str(self.in_channels) + \", \" + str(self.out_channels)\n",
    "            +\", kernel size=\" + str(self.kernel_size) + \", stride=\" + str(self.stride)\n",
    "            +\", padding=\" + str(self.padding) + \", bias=\" +str(self.bias)\n",
    "        )\n",
    "        return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# x = torch.zeros(5, 3, 6, 6)\n",
    "# for n in range(5):\n",
    "#     for c in range(3):\n",
    "#         for h in range(1, 5):\n",
    "#             for w in range(1, 5):\n",
    "#                 x[n, c, h, w] = np.random.randint(1, 10, 1)[0]\n",
    "# k = torch.zeros(2, 3, 3, 3)\n",
    "# for c in range(3):\n",
    "#     for h in range(3):\n",
    "#         for w in range(0, 3, 2):\n",
    "#             if w == 0:\n",
    "#                 k[0, c, h, w] = 1\n",
    "#                 k[1, c, w, h] = 1\n",
    "#             else:\n",
    "#                 k[0, c, h, w] = -1\n",
    "#                 k[1, c, w, h] = -1\n",
    "# \n",
    "# print(k)\n",
    "# print(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def __apply_padding(x, padding, in_channels):\n",
    "    return torch.tensor([[[[0 if w == 0 or h == 0 or w == x.shape[-1]+2*padding[1]-1 or h == x.shape[-2]+2*padding[0]-1 else x[n, c, h-1, w-1] for w in range(x.shape[-1]+2*padding[1])] for h in range(x.shape[-2]+2*padding[0])] for c in range(in_channels)]for n in range(x.shape[0])])\n",
    "# x = torch.ones(5, 3, 6, 6)\n",
    "# print(__apply_padding(x, (1,1), 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def test_conv(x_pad, i, j, weight):\n",
    "    sub = torch.narrow(torch.narrow(x_pad , -2, j, 3), -1, i, 3)\n",
    "    print(sub)\n",
    "    t = torch.multiply(torch.narrow(torch.narrow(x_pad , -2, j, 3), -1, i, 3), weight)\n",
    "    print(t.shape)\n",
    "    return [torch.sum(t[i]) for i in range(t.shape[0])]\n",
    "\n",
    "\n",
    "# print(\"narrow\")\n",
    "# print(torch.narrow(x , 2, 0, 3))\n",
    "# print(\"narrow\")\n",
    "# print(torch.narrow(torch.narrow(x , -2, 0, 3), -1, 0, 3))\n",
    "# print(test_conv(x, 0, 0, k))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# practice_loader = torch.utils.data.DataLoader(MNIST_train, batch_size=1, shuffle=False)\n",
    "# c_in = 1\n",
    "# c_out = 2\n",
    "# kernel = (3,4)\n",
    "# stride = (2,1)\n",
    "# padding = (1,2)\n",
    "# my_conv = MyConv2d(c_in, c_out, kernel_size=kernel, stride=stride, padding=padding, bias=False)\n",
    "# nn.init.uniform_(my_conv.weight.data, -1, 1)\n",
    "# n = next(iter(practice_loader))[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# out = my_conv(n)\n",
    "# print(out)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "source": [
    "## 3. Use MyConv2d inside a neural network model\n",
    "\n",
    "Just a very basic neural network so that we can test our conv layer on MNIST data. It consists of: \n",
    "\n",
    "- One 2d Convolutional layer (ours or Pytorch's) (see ``conv_type`` parameter)\n",
    "- A tanh activation function\n",
    "- One 2d MaxPooling layer to reduce the size of the image and therefore reduce the number of parameters\n",
    "- One Fully Connected layer with 10 outputs for the 10 classes of the MNIST dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple net with only one conv layer and one fc layer. conv layer can be ours or Pytorch's. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels:int,\n",
    "        out_channels:int,\n",
    "        kernel_size = 3, \n",
    "        stride = 1, \n",
    "        padding = 0, \n",
    "        bias:bool = False,\n",
    "        conv_type:str = 'custom',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        in_channels: Number of input channels \n",
    "        out_channels: Number of input channels (number of filters)\n",
    "        kernel_size: Size of your filter (either a int or a tuple of int)\n",
    "        stride: Length of the kernel's jumps  (either a int or a tuple of int)\n",
    "        padding: How many pixels do you add around the border of your image (either a int or a tuple of int)\n",
    "        bias: Should we add a bias parameter to each filter or not?\n",
    "        conv_type: Should we use MyConv2d or nn.Conv2d? \n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        # Make sure these parameters are all pairs of int\n",
    "        kernel_size = int_to_pair(kernel_size)\n",
    "        stride = int_to_pair(stride)\n",
    "        padding = int_to_pair(padding)\n",
    "        # Use MyConv2d\n",
    "        if conv_type == 'custom':\n",
    "            self.conv1 = MyConv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        # Use pytorch's Conv2d\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        # Formulas for output shape (Hope you won't look at that before answering the first question in the cell above hehe)\n",
    "        H_out = (20 + 2*padding[0] - kernel_size[0]) // stride[0] + 1 \n",
    "        W_out = (20 + 2*padding[1] - kernel_size[1]) // stride[1] + 1 \n",
    "        \n",
    "        # We divide by 2 here because we know that we will apply a pooling layer\n",
    "        self.fc2 = nn.Linear((H_out//2)*(W_out//2)*out_channels, 10)\n",
    "\n",
    "        # Where we will store all the weight values so that we can compare between Conv2d and MyConv2d\n",
    "        self.conv1_weight_values = []\n",
    "        self.conv1_bias_values = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, C_in, H, W)\n",
    "        out = F.max_pool2d(F.tanh(self.conv1(x)), 2)\n",
    "        out = out.view(-1, out.shape[-3]*out.shape[-2]*out.shape[-1])\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "source": [
    "## 4. Test MyConv2d by comparing it to nn.Conv2d\n",
    "\n",
    "We compare:\n",
    "\n",
    "- The successive weight values throughout the training (after each batch)\n",
    "- The successive training loss (after each epoch)\n",
    "\n",
    "We absolutely don't care about the model performance itself nor if the model overfits, underfits etc. We just want to test our custom conv layer\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "- Your implementation will necessarily be much slower than Pytorch's implementation. That's normal. To give you an idea, my implementation is 3-4 times slower than Pytorch's\n",
    "- As reminded below: You can play with the following parameters if you want, especially for debugging purpose **BUT BEFORE SUBMITTING YOUR NOTEBOOK:**\n",
    "  - set ``n_epochs`` to ``4`` \n",
    "  - set ``c_out`` to something ``>= 2`` before submitting your (run) notebook \n",
    "  - set ``kernel`` to something like ``(n1, n2)`` with ``n1`` not equal to ``n2`` before submitting your (run) notebook \n",
    "  - set ``stride`` to something like ``(n3, n4)`` with ``n3`` not equal to ``n4`` before submitting your (run) notebook \n",
    "  - set ``padding`` to something like ``(n5, n6)`` with ``n5`` not equal to ``n6`` before submitting your (run) notebook \n",
    "  - set ``bias`` to ``True`` before submitting your (run) notebook \n",
    "\n",
    "\n",
    "## TODO\n",
    "\n",
    "1. According to you, why is your implementation slower? (There could be multiple reasons)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n",
      "\n",
      " ========= Training using our Conv2d =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\erik_\\pycharmprojects\\deeplearning\\venv\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-10-6e70374b7439>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[0moptimizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moptim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSGD\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel01\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m training_loop(\n\u001B[0m\u001B[0;32m     57\u001B[0m     \u001B[0mn_epochs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mn_epochs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[0moptimizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-2-72fb5fb9ec0d>\u001B[0m in \u001B[0;36mtraining_loop\u001B[1;34m(n_epochs, optimizer, model, loss_fn, train_loader)\u001B[0m\n\u001B[0;32m     46\u001B[0m             \u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 48\u001B[1;33m             \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimgs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     49\u001B[0m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m             \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erik_\\pycharmprojects\\deeplearning\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    726\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 727\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-9-c8c9994ad124>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     48\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     49\u001B[0m         \u001B[1;31m# x shape: (batch_size, C_in, H, W)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 50\u001B[1;33m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_pool2d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtanh\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconv1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     51\u001B[0m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfc2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erik_\\pycharmprojects\\deeplearning\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    726\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 727\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-3-17ef1b8e6c3a>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     72\u001B[0m         \u001B[1;31m# shape of x: (N, C_in, H_in, W_in)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutput_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__get_output_size\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 74\u001B[1;33m         \u001B[0mx_pad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__apply_padding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     75\u001B[0m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutput_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     76\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mn\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutput_size\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-3-17ef1b8e6c3a>\u001B[0m in \u001B[0;36m__apply_padding\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__apply_padding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mw\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mh\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mw\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mh\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0min_channels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;32mfor\u001B[0m \u001B[0mn\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     57\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-3-17ef1b8e6c3a>\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__apply_padding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mw\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mh\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mw\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mh\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0min_channels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;32mfor\u001B[0m \u001B[0mn\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     57\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-3-17ef1b8e6c3a>\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__apply_padding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mw\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mh\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mw\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mh\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0min_channels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;32mfor\u001B[0m \u001B[0mn\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     57\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-3-17ef1b8e6c3a>\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__apply_padding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mw\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mh\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mw\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mh\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0min_channels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;32mfor\u001B[0m \u001B[0mn\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     57\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-3-17ef1b8e6c3a>\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__apply_padding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mw\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mh\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mw\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mh\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0min_channels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;32mfor\u001B[0m \u001B[0mn\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     57\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# DONT USE GPU!!! IT WOULD REQUIRE USING register_buffer TO MOVE THE MODEL CORRECTLY TO\n",
    "# THE GPU. (See https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer)\n",
    "# And you probably don't want to spend the entire week learning how to use this properly\n",
    "device = torch.device('cpu')\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "# We set shuffle to False so that we can compare both models more accurately\n",
    "train_loader = torch.utils.data.DataLoader(MNIST_train, batch_size=32, shuffle=False)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# We don't care about these parameters here in this assignment\n",
    "n_epochs = 4    # Just high enough to make sure weight values don't diverge (you can temporarily reduce this number for debugging purpose)\n",
    "lr = 0.1\n",
    "c_in = 1        # Grey scale images so c_in = 1\n",
    "\n",
    "# You can play with the following parameters if you want, especially for debugging purpose BUT BEFORE SUBMITTING YOUR NOTEBOOK:\n",
    "# - set c_out to something >= 2 before submitting your notebook\n",
    "# - set kernel to something like (n1, n2) with n1 not equal to n2\n",
    "# - set stride to something like (n3, n4) with n3 not equal to n4\n",
    "# - set padding to something like (n5, n6) with n5 not equal to n6\n",
    "# - set bias to True\n",
    "c_out = 2\n",
    "kernel = (3,4)\n",
    "stride = (2,1)\n",
    "padding = (1,2)\n",
    "bias = True \n",
    "\n",
    "model01 = MyNet(c_in, c_out, kernel_size=kernel, padding=padding, stride=stride, bias=bias, conv_type='custom').to(device=device)\n",
    "model02 = MyNet(c_in, c_out, kernel_size=kernel, padding=padding, stride=stride, bias=bias, conv_type='pytorch').to(device=device) \n",
    "\n",
    "# Initialize all our weights\n",
    "nn.init.uniform_(model02.conv1.weight.data, -1, 1) \n",
    "if model02.conv1.bias is not None:\n",
    "    nn.init.uniform_(model02.conv1.bias.data, -1, 1) \n",
    "nn.init.uniform_(model02.fc2.weight.data, -1, 1)\n",
    "nn.init.uniform_(model02.fc2.bias.data, -1, 1) \n",
    "\n",
    "# Make sure both models start with the same weights\n",
    "model01.conv1.weight.data = model02.conv1.weight.data.clone()\n",
    "if model01.conv1.bias is not None:\n",
    "    model01.conv1.bias.data = model02.conv1.bias.data.clone()\n",
    "model01.fc2.bias.data = model02.fc2.bias.data.clone()\n",
    "model01.fc2.weight.data = model02.fc2.weight.data.clone()\n",
    "\n",
    "# Keep track of the weight and bias values throughout the training (see also training loop and MyNet)\n",
    "model01.conv1_weight_values.append(model01.conv1.weight.data.clone().detach())\n",
    "if model01.conv1.bias is not None:\n",
    "    model01.conv1_bias_values.append(model01.conv1.bias.data.clone().detach())\n",
    "model02.conv1_weight_values.append(model02.conv1.weight.data.clone().detach())\n",
    "if model02.conv1.bias is not None:\n",
    "    model02.conv1_bias_values.append(model02.conv1.bias.data.clone().detach())\n",
    "\n",
    "print(\"\\n ========= Training using our Conv2d =========\")\n",
    "\n",
    "optimizer = optim.SGD(model01.parameters(), lr=lr)\n",
    "training_loop(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer, \n",
    "    model = model01,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "print(\"\\n ========= Training using Pytorch's Conv2d =========\")\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model02.parameters(), lr=lr)\n",
    "training_loop(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer, \n",
    "    model = model02,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n ======= MSE:     Pytorch's Conv2d    VS    MyConv2d   =========\")\n",
    "print(\"MSE weight:  \", np.mean([ float(torch.sum( (model02.conv1_weight_values[i] - model01.conv1_weight_values[i])**2 )) for i in range(len(model02.conv1_weight_values))])  )\n",
    "if bias:\n",
    "    print(\"MSE bias:    \", np.mean([ float(torch.sum( (model02.conv1_bias_values[i] - model01.conv1_bias_values[i])**2 )) for i in range(len(model02.conv1_bias_values))])  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(model01.conv1_bias_values)\n",
    "print(model01.conv1_weight_values)"
   ]
  }
 ]
}