{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0ae36e8c2cbd9e14d80419493f2540eab6c211be174ac39ce04705a74740d0d8b",
   "display_name": "Python 3.8.5 64-bit ('nglm-env': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# AutoEncoder, Variational AutoEncoder and GAN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Sequence\n",
    "import datetime\n",
    "import copy\n",
    "from os import listdir"
   ]
  },
  {
   "source": [
    "## General instructions\n",
    "\n",
    "Every two weeks you will be given an assignment related to the associated module. There are 3 weekly group sessions available to help you complete the assignments, you are invited to attend one of them each week. Attendance is not mandatory but recommended. However, assignments are graded and not submitting them or submitting them after the deadline will give you no points. The grading system is detailed [here](https://mitt.uib.no/courses/27468/pages/general-information)\n",
    "\n",
    "**FORMAT**: Jupyter notebook    \n",
    "**DEADLINE**: Sunday 16th May, 23:59\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this assignment we will go through 3 types of unsupervised neural network: AutoEncoder (AE), Variational AutoEncoder (VAE) and Generative Adversarial Network (GAN). In the first section we will also introduce a new type of layer: the transpose convolution as it is widely used in these unsupervised methods.\n",
    "\n",
    "Unsupervised have many advantages including the fact that they don't need labels but they are also harder to train... In this assignment it will be totally okay if you don't get good results, we will provide examples of expected results and we don't expect you to spend the entire 2 weeks on the parameter tuning.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Transpose convolution\n",
    "2. AutoEncoder\n",
    "3. Variational AutoEncoder **IN ANOTHER NOTEBOOK COMING SOON**\n",
    "4. GAN  **IN ANOTHER NOTEBOOK COMING SOON**\n",
    "\n",
    "## Related videos from the curriculum\n",
    "\n",
    "- [Lecture 15.1 — From PCA to autoencoders](https://www.youtube.com/watch?v=PSOt7u8u23w&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=69)\n",
    "- [Lecture 15.2 — Deep autoencoders](https://www.youtube.com/watch?v=6jhhIPdgkp0&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=70) \n",
    "- [Lecture 15.3 — Deep autoencoders for document retrieval](https://www.youtube.com/watch?v=ZCNbjpcX0yg&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=71)\n",
    "- [Lecture 15.6 — Shallow autoencoders for pre training](https://www.youtube.com/watch?v=xjlvVfEbhz4&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=74)\n",
    "- [Lecture 13 | Generative Models]\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Transpose convolution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**NOTE:** The next sections do not depend on this section, if you are stuck here you can move on to the section 2. :)\n",
    "\n",
    "--------------------\n",
    "\n",
    "In this assignment we will use a new type of layer: Transpose Convolution. This layer is typically used when we want to use a neural network to generate images (which is the case for AE, VAE and GAN). To make sure that you understand what it does we ask you to implement a simplified version of the transpose convolution operation. \n",
    "\n",
    "TransposeConvolution is NOT the inverse operation of convolution! In mathematics, deconvolution is the operation inverse to convolution! But in machine learning it is often said that TransposeConvolution layers are the symmetric or the inverse of the convolution layers (which is not mathematically true).  The name comes from the fact that the transposed convolution transposes the weight matrix and the input in the actual calculation process compared to the direct convolution.\n",
    "\n",
    "Surprisingly, Andrew did not make any video about this layer. For some ressource you can check this [blog post](https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8) or this [video](https://www.youtube.com/watch?v=QmCxqsbn5B0). Specifically, for an input tensor ``x`` of shape ``(N, C_in, H_in, W_in)`` and a weight tensor ``weights`` of shape ``(C_in, C_out, kernel_size[0], kernel_size[1])`` a transpose convolution layer returns  a tensor ``out`` of shape ``(N, C_out, H_out, W_out)`` (See the *shape* section of [torch.nn.ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=convtranspose2d#torch.nn.ConvTranspose2d) for the formula that gives ``(H_out, W_out)``)), such that:\n",
    "\n",
    "$$out[n, \\; c_{out}, \\; h_{start}:h_{end}, \\; w_{start}:w_{end}] = \\sum_{c_{in} = 0}^{C_{in}-1} x[n, \\; c_{in}, \\; h, \\; w] * weights[c_{in}, \\; c_{out}, \\; :h_{end}-h_{start}, \\; :w_{end}-w_{start}]$$\n",
    "\n",
    "For:\n",
    "\n",
    "- $n = 0 ... N - 1$\n",
    "- $c = 0 ... C_{out} - 1$\n",
    "- $h = 0 ... H_{in} - 1$\n",
    "- $w = 0 ... W_{in} - 1$\n",
    "\n",
    "\n",
    "With:\n",
    "- $h_{start} = h*stride[0]$\n",
    "- $w_{start} = w*stride[1]$\n",
    "- $h_{end} = min(H_{out}, h_{start} + kernel_size[0])$\n",
    "- $w_{end} = min(W_{out}, w_{start} + kernel_size[1])$\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Write a ``get_output_size`` function that takes as parameter an input tensor ``x`` of shape ``(N, C_in, H_in, W_in)``, a tuple of int ``kernel_size`` and a tuple of int ``stride`` and that returns the expected output spatial shape ``(H_out, W_out)`` of the transpose convolution operation. See the *shape* section of [torch.nn.ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=convtranspose2d#torch.nn.ConvTranspose2d) for the formula that gives this output shape (We ignore the padding here, so ``padding=0``)\n",
    "\n",
    "2. Write a ``apply_transpose_conv`` function that takes as parameter an input tensor ``x`` of shape ``(N, C_in, H_in, W_in)``, a weight tensor ``weights`` of shape ``(C_in, C_out, kernel_size[0], kernel_size[1])`` and a tuple of int ``stride`` and that returns ``out``, a tensor with the right shape and containing the result of the transpose convolution operation between ``x`` and ``weights``. You are of course not allowed to use [torch.nn.ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=convtranspose2d#torch.nn.ConvTranspose2d) not its functional counterpart. \n",
    "\n",
    "**NOTE:** We will not really use this function in practice so it's okay to use for loops for the sake of clarity and simplicity in this section\n",
    "\n",
    "**NOTE:** You will be able to test your functions in the next cell."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_pair(n):\n",
    "    \"\"\"\n",
    "    Return `(n, n)` if `n` is a int or `n` if `n` is already a tuple of length 2\n",
    "    \"\"\"\n",
    "    # If n is a float or integer\n",
    "    if not isinstance(n, Sequence):\n",
    "        return (int(n), int(n))\n",
    "    elif len(n) == 1:\n",
    "        return (int(n[0]), int(n[0]))\n",
    "    elif len(n) == 2:\n",
    "        return ( int(n[0]), int(n[1]) )\n",
    "    else:\n",
    "        raise ValueError(\"Please give an int or a pair of int\")\n",
    "\n",
    "def get_output_size(x, kernel_size, stride):\n",
    "    (_, _, H_in, W_in) = x.shape\n",
    "    ... #TODO! \n",
    "    return (H_out, W_out)\n",
    "\n",
    "\n",
    "def apply_transpose_conv(x, weights, stride):\n",
    "    # stride must be pairs of int\n",
    "    stride = int_to_pair(stride)\n",
    "\n",
    "    # weights\n",
    "    (_ , C_out, k_w, k_h) = weights.shape\n",
    "    kernel_size = [k_w, k_h]\n",
    "\n",
    "    # get output shape\n",
    "    (N, C_in, H_in, W_in) = x.shape\n",
    "    H_out, W_out = #TODO! \n",
    "\n",
    "    # Initialize output tensor with the right shape \n",
    "    out = torch.zeros((N, C_out, H_out, W_out))\n",
    "\n",
    "    # We will not really use this function in practice so it's okay to use \n",
    "    # for loops for the sake of clarity \n",
    "    ... #TODO! \n",
    "    return out\n",
    "\n",
    "def initialize_weights(C_in, C_out, kernel_size):\n",
    "    \"\"\"\n",
    "    Helper function for the tests in the cell below\n",
    "    \"\"\"\n",
    "    kernel_size = int_to_pair(kernel_size)\n",
    "    len_weights = C_in*C_out*kernel_size[0]*kernel_size[1]\n",
    "    weights = (torch.arange(len_weights) - len_weights/2)/10\n",
    "    return weights.reshape(C_in, C_out, kernel_size[0], kernel_size[1])"
   ]
  },
  {
   "source": [
    "### Test tranpose convolution "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1_exp = torch.Tensor([[[[ -30.,  -55.,  -55.,  -25.],\n",
    "          [ -50.,  -90.,  -90.,  -40.],\n",
    "          [ -60., -105., -105.,  -45.],\n",
    "          [ -60., -105., -105.,  -45.],\n",
    "          [ -30.,  -50.,  -50.,  -20.],\n",
    "          [ -10.,  -15.,  -15.,   -5.]],\n",
    "\n",
    "         [[   0.,    5.,    5.,    5.],\n",
    "          [  10.,   30.,   30.,   20.],\n",
    "          [  30.,   75.,   75.,   45.],\n",
    "          [  30.,   75.,   75.,   45.],\n",
    "          [  30.,   70.,   70.,   40.],\n",
    "          [  20.,   45.,   45.,   25.]]],\n",
    "\n",
    "\n",
    "        [[[ -30.,  -55.,  -55.,  -25.],\n",
    "          [ -50.,  -90.,  -90.,  -40.],\n",
    "          [ -60., -105., -105.,  -45.],\n",
    "          [ -60., -105., -105.,  -45.],\n",
    "          [ -30.,  -50.,  -50.,  -20.],\n",
    "          [ -10.,  -15.,  -15.,   -5.]],\n",
    "\n",
    "         [[   0.,    5.,    5.,    5.],\n",
    "          [  10.,   30.,   30.,   20.],\n",
    "          [  30.,   75.,   75.,   45.],\n",
    "          [  30.,   75.,   75.,   45.],\n",
    "          [  30.,   70.,   70.,   40.],\n",
    "          [  20.,   45.,   45.,   25.]]]])\n",
    "\n",
    "out2_exp = torch.Tensor([[[[ 1.87199997e+02,  3.45299988e+02,  3.44749969e+02,  1.57900024e+02],\n",
    "          [ 3.15900024e+02,  5.73850037e+02,  5.72950012e+02,  2.57650024e+02],\n",
    "          [ 3.86400024e+02,  6.86250122e+02,  6.85200073e+02,  2.99550049e+02],\n",
    "          [ 3.84599976e+02,  6.83099976e+02,  6.82049988e+02,  2.98200073e+02],\n",
    "          [ 1.99200012e+02,  3.41249969e+02,  3.40749939e+02,  1.41950043e+02],\n",
    "          [ 7.10999832e+01,  1.13749985e+02,  1.13599983e+02,  4.26499939e+01]],\n",
    "\n",
    "         [[ 1.43999786e+01,  1.52587891e-05,  5.00183105e-02, -1.43000031e+01],\n",
    "          [-2.87999954e+01, -1.14950035e+02, -1.14650032e+02, -8.58500290e+01],\n",
    "          [-1.29299973e+02, -3.44249939e+02, -3.43500000e+02, -2.14350006e+02],\n",
    "          [-1.28399994e+02, -3.42000000e+02, -3.41250000e+02, -2.13000031e+02],\n",
    "          [-1.41899979e+02, -3.40349976e+02, -3.39650024e+02, -1.97949997e+02],\n",
    "          [-9.90000000e+01, -2.26150009e+02, -2.25700012e+02, -1.26849998e+02]]],\n",
    "\n",
    "\n",
    "        [[[ 1.69199982e+02,  3.12300018e+02,  3.11750061e+02,  1.42900024e+02],\n",
    "          [ 2.85900024e+02,  5.19850098e+02,  5.18950012e+02,  2.33649994e+02],\n",
    "          [ 3.50400024e+02,  6.23250000e+02,  6.22200012e+02,  2.72549988e+02],\n",
    "          [ 3.48599945e+02,  6.20099854e+02,  6.19049927e+02,  2.71200012e+02],\n",
    "          [ 1.81199982e+02,  3.11250031e+02,  3.10750000e+02,  1.29949997e+02],\n",
    "          [ 6.50999985e+01,  1.04750008e+02,  1.04599998e+02,  3.96500015e+01]],\n",
    "\n",
    "         [[ 1.44000015e+01,  3.00000763e+00,  3.05000305e+00, -1.12999954e+01],\n",
    "          [-2.27999954e+01, -9.69499969e+01, -9.66499939e+01, -7.38500214e+01],\n",
    "          [-1.11299980e+02, -2.99249939e+02, -2.98500031e+02, -1.87350037e+02],\n",
    "          [-1.10399994e+02, -2.97000000e+02, -2.96250000e+02, -1.86000031e+02],\n",
    "          [-1.23899971e+02, -2.98350037e+02, -2.97650024e+02, -1.73950012e+02],\n",
    "          [-8.69999924e+01, -1.99150024e+02, -1.98700012e+02, -1.11850006e+02]]]])\n",
    "\n",
    "N = 2\n",
    "H_in = 4\n",
    "W_in = 3\n",
    "C_in = 5\n",
    "C_out = 2\n",
    "\n",
    "kernel_size = (3,2)\n",
    "stride = 1\n",
    "\n",
    "weights = initialize_weights(C_in, C_out, kernel_size)\n",
    "x1 = torch.ones((N, C_in, H_in, W_in))*10\n",
    "x2 = torch.arange(120).reshape(N, C_in, H_in, W_in)/10 - 60\n",
    "\n",
    "print(\"weights.shape:  \", weights.shape)\n",
    "print(\"x.shape:        \", x1.shape)\n",
    "print(\"shape expected: \", out1_exp.shape)\n",
    "\n",
    "out1 = apply_transpose_conv(x1, weights, stride)\n",
    "print(\"out1.shape:     \", out1.shape)\n",
    "out2 = apply_transpose_conv(x2, weights, stride)\n",
    "print(\"out2.shape:     \", out2.shape)\n",
    "\n",
    "print(\"\\nout1 - out1_exp:\\n \", out1 - out1_exp)\n",
    "print(\"\\nout2 - out2_exp:\\n \", out2 - out2_exp)"
   ]
  },
  {
   "source": [
    "# 2. AutoEncoder\n",
    "\n",
    "*related videos from the curriculum*\n",
    "\n",
    "- [Lecture 15.1 — From PCA to autoencoders](https://www.youtube.com/watch?v=PSOt7u8u23w&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=69)\n",
    "- [Lecture 15.2 — Deep autoencoders](https://www.youtube.com/watch?v=6jhhIPdgkp0&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=70) \n",
    "- [Lecture 15.3 — Deep autoencoders for document retrieval](https://www.youtube.com/watch?v=ZCNbjpcX0yg&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=71)\n",
    "- [Lecture 15.6 — Shallow autoencoders for pre training](https://www.youtube.com/watch?v=xjlvVfEbhz4&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=74)\n",
    "- [Lecture 13 | Generative Models](https://www.youtube.com/watch?v=5WoItGTWV54&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=17) (from 20:40 to 27:05)\n",
    "\n",
    "An AutoEncoder (AE) is a neural network that is composed of 2 sub-networks: an Encoder and a Decoder. AE can have many purposes but commonly the main objective is to efficiently represent the data that lies on a non-linear manifold. From this point of view, the AutoEncoder can be seen as a generalization of PCA for data that lies on non-linear manifolds as explained in this [video](https://www.youtube.com/watch?v=PSOt7u8u23w&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=69) of the curriculum.\n",
    "\n",
    "If this is the objective then the Decoder part can be thrown away once the training is done and we can simply use the encoder part to project the data into a latent space of lower dimension ``z_dim`` (in this section, ``z_dim`` will typically be 15 or 30 while the images are 20x20 (=400)). Indeed the Decoder is here just to make sure that the encoding is well done and ensure and faithful representation of the data by comparing the reconstructed (i.e decoded) instances of the compressed (i.e encoded) data with the original data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Modules \n",
    "\n",
    "In the cell below are defined the following modules that we will need in this section\n",
    "\n",
    "1. **MyEncoder**\n",
    "    - input: image\n",
    "    - output: tensor `z` in latent space (lower dimension than input space)\n",
    "1. **MyDecoder**\n",
    "    - input: tensor `z` in latent space (lower dimension than image space)\n",
    "    - output: reconstructed image\n",
    "1. **MyAE**\n",
    "    - input: image\n",
    "    - output reconstructed image after reduction to latent space\n",
    "    - attributes:\n",
    "      - self.encoder = MyEncoder(z_dim)\n",
    "      - self.decoder = MyDecoder(z_dim)\n",
    "1. **LeNet5ish**\n",
    "    Regular image classifier\n",
    "    - input: image\n",
    "    - output: label predicted\n",
    "1. **MyClassifier**\n",
    "    Classifier on compressed images:\n",
    "    - input: tensor `z` in latent space (lower dimension than image space)\n",
    "    - output: label predicted\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Take a look at the ``MyEncoder``, ``MyDecoder`` and ``MyAE`` modules. Do the Encoder and Decoder seem really different from any other neural networks so far?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module: \n",
    "    - input: image\n",
    "    - output: tensor `z` in latent space (lower dimension than input space)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=6, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=5, kernel_size=5, stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=5, out_channels=4, kernel_size=4, stride=1)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        out = torch.relu(self.conv1(x))\n",
    "        out = torch.relu(self.conv2(out))\n",
    "        out = torch.relu(self.conv3(out))\n",
    "        out = out.view(N, -1)\n",
    "        out = torch.relu(self.fc3(out))\n",
    "        return out\n",
    "\n",
    "class MyDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder module: \n",
    "    - input: tensor `z` in latent space (lower dimension than image space)\n",
    "    - output: reconstructed image\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__() \n",
    "        c1 = 3\n",
    "        self.fc1 = nn.Linear(z_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 18*18)\n",
    "        self.transconv3 = nn.ConvTranspose2d(in_channels=1, out_channels=1,  kernel_size=3, stride=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, z_dim = x.shape\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = out.view(N, 1, 18, 18)\n",
    "        out = torch.sigmoid(self.transconv3(out))\n",
    "        return out\n",
    "\n",
    "class MyAE(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoEncoder\n",
    "    - input: image\n",
    "    - output reconstructed image after reduction to latent space\n",
    "\n",
    "    attributes\n",
    "    - self.encoder = MyEncoder(z_dim)\n",
    "    - self.decoder = MyDecoder(z_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__() \n",
    "        self.encoder = MyEncoder(z_dim)\n",
    "        self.decoder = MyDecoder(z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.z = self.encoder(x)\n",
    "        out = self.decoder(self.z)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LeNet5ish(nn.Module):\n",
    "    \"\"\"\n",
    "    Regular image classifier\n",
    "    - input: image\n",
    "    - output: label predicted\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_labels=10):\n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.fc3 = nn.Linear(in_features=864, out_features=120)\n",
    "        self.fc4 = nn.Linear(in_features=120, out_features=n_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.view(N, -1)\n",
    "        out = F.relu(self.fc3(out))\n",
    "        out = F.relu(self.fc4(out))\n",
    "        return out\n",
    "\n",
    "class MyClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier on compressed images:\n",
    "    - input: tensor `z` in latent space (lower dimension than image space)\n",
    "    - output: label predicted\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim, n_labels=10):\n",
    "        super().__init__() \n",
    "        c1 = 12\n",
    "        self.fc1 = nn.Linear(z_dim, 64)\n",
    "        self.transconv1 = nn.ConvTranspose2d(in_channels=1,  out_channels=c1, kernel_size=3, stride=1)\n",
    "        self.transconv2 = nn.ConvTranspose2d(in_channels=c1, out_channels=1,  kernel_size=3, stride=1)\n",
    "        self.fc2 = nn.Linear(144, n_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, z_dim = x.shape\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = out.view(N, 1, 8, 8)\n",
    "        out = F.relu(self.transconv1(out)) \n",
    "        out = F.relu(self.transconv2(out))\n",
    "        out = out.view(N, -1)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Utils \n",
    "\n",
    "Some useful functions:\n",
    "\n",
    "- **load_MNIST**: Return MNIST train and val dataset\n",
    "- **training_classifier**: Usual training loop for a classifier\n",
    "- **validate**: Compute classification accuracies on both train and val dataset\n",
    "- **plot_true_VS_reconstructed**: Plot side by side original images with their reconstructed counterparts using a trained AE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = transforms.Compose([\n",
    "    transforms.CenterCrop(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.2475, 0.3892),\n",
    "]) \n",
    "\n",
    "\n",
    "def load_MNIST(data_path='../data/', transform = preprocessor, labels_kept=[0,1,3,4,8]):\n",
    "    \"\"\"\n",
    "    Return MNIST train and val dataset\n",
    "    \"\"\"\n",
    "    MNIST_train = datasets.MNIST(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,   \n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    MNIST_val = datasets.MNIST(\n",
    "        data_path, \n",
    "        train=False,      \n",
    "        download=True,   \n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    print('Size of the original training dataset: ', len(MNIST_train))\n",
    "    print('Size of the original validation dataset: ', len(MNIST_val))\n",
    "\n",
    "    if len(labels_kept) <10:\n",
    "        MNIST_train_reduced = [(img, labels_kept.index(label)) for img, label in MNIST_train if label in labels_kept]\n",
    "        MNIST_val_reduced = [(img, labels_kept.index(label)) for img, label in MNIST_val if label in labels_kept]\n",
    "\n",
    "        print('Size of the reduced training dataset: ', len(MNIST_train_reduced))\n",
    "        print('Size of the reduced validation dataset: ', len(MNIST_val_reduced))\n",
    "    else:\n",
    "        MNIST_train_reduced = MNIST_train\n",
    "        MNIST_val_reduced = MNIST_val\n",
    "\n",
    "    return MNIST_train_reduced, MNIST_val_reduced\n",
    "\n",
    "\n",
    "def training_classifier(n_epochs, optimizer, model, loss_fn, train_loader, device=None):\n",
    "    \"\"\"\n",
    "    Usual training loop for a classifier\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device=device) \n",
    "            labels = labels.to(device=device) \n",
    "            batch_size = imgs.shape[0]\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 1 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))\n",
    "\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    \"\"\"\n",
    "    Compute classification accuracies on both train and val dataset\n",
    "    \"\"\"\n",
    "    accdict = {}\n",
    "    model.eval()\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "        accdict[name] = correct / total\n",
    "    return accdict\n",
    "\n",
    "def plot_true_VS_reconstructed(ae, imgs):\n",
    "    \"\"\"\n",
    "    Plot side by side original images with their reconstructed counterpart using a trained AE\n",
    "    \"\"\"\n",
    "    ae.eval()\n",
    "    N_img = 25\n",
    "    fig, axs = plt.subplots(nrows=5, ncols=10, figsize=(10,6), sharex=True, sharey=True)\n",
    "    for i, img in enumerate(imgs[:N_img]):\n",
    "        with torch.no_grad():\n",
    "            out = ae(img.unsqueeze(0))\n",
    "            # True image\n",
    "            axs.flat[2*i].imshow(img.permute(1, 2, 0), cmap='Greys')\n",
    "            # Reconstruction\n",
    "            axs.flat[2*i + 1].imshow(out.squeeze(0).permute(1, 2, 0), cmap='Greys') \n",
    "            # Set ax title for the first row\n",
    "            if i<5:\n",
    "                axs.flat[2*i].set_title(\"True\\nimage\")\n",
    "                axs.flat[2*i + 1].set_title(\"AE recon-\\nstruction\")\n",
    "    return fig, axs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "labels_kept = [i for i in range(10)]\n",
    "data_train, data_val = load_MNIST(labels_kept=labels_kept)\n",
    "imgs_train = [img for img, _ in data_train]\n",
    "label_train = [label for _, label in data_train]\n",
    "imgs_val = [img for img, _ in data_val]\n",
    "label_val = [label for _, label in data_val]"
   ]
  },
  {
   "source": [
    "### Training loop of an AutoEncoder\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Write a function ``training_ae`` (you can get inspired by the ``training_classifier`` function in the utils cell above) that trains an auto-encoder. The objective is that the encoder part of the AE gets good at summarizing the data in the latent space and that the decoder gets good at reconstructing the images from the lower dimensional vectors in the latent space.\n",
    "\n",
    "Note that:\n",
    "- There is no label in the dataset (so no label in ``train_loader`` neither)\n",
    "- The loss function is computed by comparing the outputs with the original images. We will typically call this function with loss_fn = `` nn.MSELoss()`` so that each reconstructed pixel is compared to its original couterpart."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_ae(n_epochs, optimizer, model, loss_fn, train_loader, device=None):\n",
    "    \"\"\"\n",
    "    Train an AE. No labels required\n",
    "    \"\"\"\n",
    "    #TODO!"
   ]
  },
  {
   "source": [
    "### Training your AutoEncoder\n",
    "\n",
    "Run the cell below to train your AE. You can play with the parameter if you want/need.\n",
    "Keep in mind that training unsupervised model is not easy and that it is okay in this assignment if you don't get good results at all, we will provide examples of results so that you can still answer the questions).\n",
    "For this assignment (and in its specific settings) we can say that your AE learns well if your training loss is lower than 0.6. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "z_dim = 15\n",
    "\n",
    "ae = MyAE(z_dim=z_dim)\n",
    "ae.to(device=device)\n",
    "\n",
    "train_loader_imgs = torch.utils.data.DataLoader(imgs_train, batch_size=512, shuffle=True)\n",
    "val_loader_imgs = torch.utils.data.DataLoader(imgs_val, batch_size=512, shuffle=True)\n",
    "\n",
    "lr = 0.5\n",
    "momentum = 0.6\n",
    "\n",
    "optimizer = optim.SGD(ae.parameters(), lr=lr, momentum=momentum)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "training_ae(\n",
    "    n_epochs = 30,\n",
    "    optimizer = optimizer,\n",
    "    model = ae,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_imgs,\n",
    ")"
   ]
  },
  {
   "source": [
    "### Plot original images VS reconstruction\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "Analyse your results (regardless of how good/bad they might be)\n",
    "\n",
    "1. Are you satisfied by the reconstructions? If not, what seems to be the problem? Mode collapse? Overfitting? Underfitting? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ae.to(device=torch.device('cpu')) \n",
    "fig, axs = plot_true_VS_reconstructed(ae, imgs_train)\n",
    "fig.suptitle(\"Training dataset\", fontsize=15)\n",
    "plt.show()\n",
    "fig, axs = plot_true_VS_reconstructed(ae, imgs_val)\n",
    "fig.suptitle(\"Validation dataset\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "\n",
    "\n",
    "![Example of AE results on the validation dataset (see train01 image)](./train01.png)\n",
    "\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "Analyse the results in the image above.\n",
    "\n",
    "1. Without looking at the true image, would you say that the reconstructed image looks good?\n",
    "1. Now take a closer look at the bottom right pair. What can you say? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Compress images using the Encoder part of our trained AE\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "\n",
    "1. Use the ``transform_images`` function defined in the cell below to compress ``train_loader_imgs`` and ``val_loader_imgs`` and store them in ``compressed_imgs_train`` and ``compressed_imgs_val``. To do so call the function with the encoder part of our AE (accessible using ``ae.encoder``)\n",
    "2. We have divided by more than 10 the size of the original data, cite some obvious advantages of this compression. \n",
    "3. Recalling this [video](https://www.youtube.com/watch?v=ZCNbjpcX0yg&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=71), this other [video](https://www.youtube.com/watch?v=xjlvVfEbhz4&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=74) and this [lecture](https://www.youtube.com/watch?v=5WoItGTWV54&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=16) (from 20:40 to 27:05 ) from the curriculum and the exercise \"*CNN: visualization and interpretation, section 3. Last layer: Nearest neighbors*\", what could be another application of these compressed data?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_images(model, dataloader, device=None):\n",
    "    \"\"\"\n",
    "    Apply ``model`` to ``dataloader`` and returns the output \n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "    model.eval()\n",
    "    transformed_imgs = []\n",
    "    for imgs in dataloader:\n",
    "        with torch.no_grad():\n",
    "            imgs = imgs.to(device=device) \n",
    "            outputs = model(imgs)\n",
    "            transformed_imgs.append(outputs.clone().detach())\n",
    "    transformed_imgs = torch.cat(transformed_imgs)\n",
    "    return transformed_imgs\n",
    "\n",
    "\n",
    "ae.to(device=device)\n",
    "\n",
    "train_loader_imgs = torch.utils.data.DataLoader(imgs_train, batch_size=256, shuffle=False) # Keep shuffle to False here otherwise imgs won't correspond to labels anymore\n",
    "val_loader_imgs = torch.utils.data.DataLoader(imgs_val, batch_size=256, shuffle=False)     # Keep shuffle to False here otherwise imgs won't correspond to labels anymore\n",
    "\n",
    "# Use the encoder part of our ae to compress our images\n",
    "compressed_imgs_train = #TODO!\n",
    "compressed_imgs_val = #TODO!\n",
    "\n",
    "# Re-associate compressed images with their corresponding labels\n",
    "data_compressed_train = list(zip(compressed_imgs_train, label_train))\n",
    "data_compressed_val = list(zip(compressed_imgs_val, label_val))"
   ]
  },
  {
   "source": [
    "### Classification on compressed images\n",
    "\n",
    "In the cell below we train ``MyClassifier`` (defined in the \"Modules\" subsection at the beginning of this section), a classifier that takes as input a tensor `z` in the latent space (lower dimension than the image space) and find the label corresponding to the original image.\n",
    "\n",
    "In the next cell we evaluate the performance of our classifier.\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Recalling that we started from 20x20 (=400) images that were compressed into a 15 dimensional space, are you satisfied by the classification performance? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training on device {device}.\")\n",
    "epoch = 30\n",
    "momentum = 0.\n",
    "lr=0.1\n",
    "\n",
    "\n",
    "train_loader_compressed = torch.utils.data.DataLoader(data_compressed_train, batch_size=512, shuffle=True)\n",
    "val_loader_compressed = torch.utils.data.DataLoader(data_compressed_val, batch_size=512, shuffle=True)\n",
    "\n",
    "classifier_compressed = MyClassifier(z_dim=z_dim, n_labels=len(labels_kept))\n",
    "classifier_compressed.to(device=device) \n",
    "optimizer = optim.SGD(classifier_compressed.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\" ============== Training Classifier on compressed images ============== \")\n",
    "training_classifier(\n",
    "    n_epochs = epoch,\n",
    "    optimizer = optimizer,\n",
    "    model = classifier_compressed,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_compressed,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ============== Classification on compressed images ============== \")\n",
    "_ = validate(\n",
    "    model = classifier_compressed,\n",
    "    train_loader = train_loader_compressed,\n",
    "    val_loader = val_loader_compressed,\n",
    ")"
   ]
  },
  {
   "source": [
    "### Reconstruct images using the Decoder part of our trained AE\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "\n",
    "1. Use the ``transform_images`` function defined earlier to reconstruct ``train_loader_compressed_imgs`` and ``val_loader_compressed_imgs`` and store them in ``reconstructed_imgs_train`` and ``reconstructed_imgs_val``. To do so call the function with the decoder part of our AE (accessible using ``ae.decoder``)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_compressed_imgs = torch.utils.data.DataLoader(compressed_imgs_train, batch_size=256, shuffle=False)   # Keep shuffle to False here otherwise imgs won't correspond to labels anymore\n",
    "val_loader_compressed_imgs = torch.utils.data.DataLoader(compressed_imgs_val, batch_size=256, shuffle=False)       # Keep shuffle to False here otherwise imgs won't correspond to labels anymore\n",
    "\n",
    "# Use the decoder part of our ae to reconstruct our compressed images\n",
    "reconstructed_imgs_train = #TODO!\n",
    "reconstructed_imgs_val = #TODO!\n",
    "\n",
    "# Re-associate reconstructed images with their corresponding labels\n",
    "data_reconstructed_train = list(zip(reconstructed_imgs_train, label_train))\n",
    "data_reconstructed_val = list(zip(reconstructed_imgs_val, label_val))"
   ]
  },
  {
   "source": [
    "### Classification on reconstructed images VS true images\n",
    "\n",
    "In the cell below we train 2 instances of ``LeNet5ish`` (defined in the \"Modules\" subsection at the beginning of this section), a regular classifier that takes an image and find its corresponding label. The first instance is trained on the reconstructed images and the second one is trained on the original images\n",
    "\n",
    "In the next cell we evaluate the performance of our classifiers.\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Compare the performance of the 2 models. Are you satisfied with the classification on the reconstructed images?\n",
    "2. If so, can we throw away our original images (and keep only our trained AE and compressed images)? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "epoch = 20\n",
    "momentum = 0.7\n",
    "lr=0.07\n",
    "\n",
    "print(\" ============== Training Classifier on reconstructed images ============== \")\n",
    "\n",
    "train_loader_reconstructed = torch.utils.data.DataLoader(data_reconstructed_train, batch_size=512, shuffle=True)\n",
    "val_loader_reconstructed = torch.utils.data.DataLoader(data_reconstructed_val, batch_size=512, shuffle=True)\n",
    "\n",
    "classifier_reconstructed = LeNet5ish(n_labels=len(labels_kept))\n",
    "classifier_reconstructed.to(device=device) \n",
    "optimizer = optim.SGD(classifier_reconstructed.parameters(), lr=lr, momentum=momentum)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_classifier(\n",
    "    n_epochs = epoch,\n",
    "    optimizer = optimizer,\n",
    "    model = classifier_reconstructed,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_reconstructed,\n",
    ")\n",
    "\n",
    "print(\" ============== Training Classifier on true images ============== \")\n",
    "\n",
    "train_loader_true = torch.utils.data.DataLoader(data_train, batch_size=512, shuffle=True)\n",
    "val_loader_true = torch.utils.data.DataLoader(data_val, batch_size=512, shuffle=True)\n",
    "\n",
    "classifier_true = LeNet5ish(n_labels=len(labels_kept))\n",
    "classifier_true.to(device=device) \n",
    "optimizer = optim.SGD(classifier_true.parameters(), lr=lr, momentum=momentum)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_classifier(\n",
    "    n_epochs = epoch,\n",
    "    optimizer = optimizer,\n",
    "    model = classifier_true,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_true,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ============== Classification on reconstructed images ============== \")\n",
    "_ = validate(\n",
    "    model = classifier_reconstructed,\n",
    "    train_loader = train_loader_reconstructed,\n",
    "    val_loader = val_loader_reconstructed,\n",
    ")\n",
    "\n",
    "print(\" ============== Classification on true images ============== \")\n",
    "_ = validate(\n",
    "    model = classifier_true,\n",
    "    train_loader = train_loader_true,\n",
    "    val_loader = val_loader_true,\n",
    ")"
   ]
  },
  {
   "source": [
    "## 3. Variational AutoEncoder  \n",
    " \n",
    "IN ANOTHER NOTEBOOK COMING SOON"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4. GAN\n",
    " \n",
    "IN ANOTHER NOTEBOOK COMING SOON"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}